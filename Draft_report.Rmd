---
title: "Final Project"
subtitle: "Water Temperature"
author: "Zhanylai Turatkhan kyzy, Julia Kagiliery, Yilun Zhu"
output: pdf_document
geometry: margin=2.54cm
---

```{r, echo=FALSE}
library(ggplot2)
library(readxl)
library(cowplot)
library(tidyverse)
library(lubridate)
library(dplyr)
library(knitr)
library(caret)
library(forecast)
library(zoo)
library(Kendall)
library(tseries)
library(outliers)
library(smooth)
```

**Introduction**


**Motivation/relevance of the study**
Global ocean temperature is on the rise which has significant implications for 
this dynamic and productive biome. Among the many services the ocean provides, 
primary production (of oxygen) is among the most important. In fact, 
approximately 50% of primary production of oxygen comes from marine 
phytoplankton. It has been proven (largely through work by Duke University 
Marine Lab's Dr. Zacakry Johnson and Dr. Dana Hunt) that these microbial 
systems change significantly with seasonal changes (which include dominantly temperature changes, but also isolation and day length changes). These changes 
alter the way that the ocean cycles nutrients, stores carbon, and produces 
oxygen. Hence, temperature is an important variable in the consideration of 
much ocean modeling. This highlights the importance of accurate temperature 
prediction. Understanding what future climate and temperature looks like 
allows for better prediction of what other ocean cycles will look like. 

**Objectives**

Our objective is to accurately model the monthly temperature of Piver's 
Island Coastal Observatory and produce reasonable forecasting at appropriate 
time scales.

**Dataset Information**

The following data comes from a long running time series study out at Piver's 
Island Coastal Observatory (PICO) which is located at the Duke University 
Marine Lab. The sytudy monitors the ambient conditions such as turbidity, pH, temperature, and salinity. For this study, only temperature was able to be 
included. The temperature is reported as mean monthly temperature. 

**Methodology / Analysis**

There are first a few important considerations as we get into modeling. The 
first is that this data is actually measured approximately weekly, meaning 
sometimes the lab samples multiple times in a week or skips a week. In order 
to account for this irregular time series frequency, we aggregated the data 
into monthly means to avoid unnecessary complications and misalignment of our 
time series. 
The second consideration we would like to acknowledge that climate change is 
certainly a a factor that plays a role in shifting trends, seasonal components, 
and general variability which may not be accurately reflected in our current 
data set which spans only approximately 11 years. Though the temperature in the 
region looks highly predictable, future predictions must be cognizant that large prediction horizons are unreasonable. 

**Description**

**1. SARIMA model:** The Seasonal Autoregressive Integrated Moving Average 
(SARIMA) model expands upon the ARIMA framework to address seasonality in 
univariate data sets. It incorporates both non-seasonal and seasonal elements 
in its predictions, allowing it to capture complex patterns that recur over 
fixed periods.
**2. Arima+Fourier:** This approach combines the ARIMA model with Fourier series 
to enhance the modeling of time series with complex seasonal patterns. 
ARIMA captures the autocorrelations in the data, while the Fourier terms allow 
for the approximation of seasonal cycles of various lengths and complexities. 
This hybrid model is especially useful when the seasonality is not strictly 
periodic or involves multiple frequencies.
**3. TBATS model:** Designed for forecasting time series with intricate seasonal patterns, the TBATS model employs exponential smoothing as its core technique. 
It thoroughly explores a variety of specifications, including those with and 
without a Box-Cox transformation, the presence or absence of a trend, trend 
damping options, and an ARIMA(p,q) component for the residuals. The model also evaluates different harmonic levels for seasonalities. The best-fitting model 
is determined by the lowest Akaike Information Criterion (AIC).
**4. ETS model:** This univariate forecasting method, known as Exponential 
Smoothing State Space Model (ETS), emphasizes trend and seasonality components 
within the time series data. It is particularly adept at capturing patterns 
that evolve over time.
**5. SSES model:** The State Space Exponential Smoothing (SSES) model extends 
the classic exponential smoothing approach by incorporating distribution 
assumptions about the error terms, which aids in the computation of prediction 
intervals. It considers both additive and multiplicative error structures 
within the state space modeling framework.
**6. Neural Network:** The Neural Network model facilitates the identification 
of complex and nonlinear relationships between the dependent variable and its predictors. Its versatile architecture can adapt to a wide array of data 
patterns.

```{r, echo=FALSE}
#file_path <- file.path(getwd(), "Monthly Data_Temperature2011-2021.xlsx")
#print(file_path)
```


```{r, echo=FALSE}
#We important the monthly temperature data
file_path <- "/home/guest/TSA_Janka/WaterTemperature_TSA_FinalProject/Monthly Data_Temperature2011-2021.xlsx"
temperature_data <- read_excel(file_path, sheet = "Sheet1")
```

```{r, echo=FALSE}
#Transforming the data into a time series which starts in Jan of 2011
temperature_ts <- ts(temperature_data$Temperature,
                     start = c(2011, 1), frequency = 12)
```

```{r, echo=FALSE}
#Upon initial examination, the data appears to be strongly seasonal with very little trend.
plot(temperature_ts, main = "Original dataset")
```
The temperature shows cyclical behavior with peaks and troughs that correspond 
to expected seasonal variations. The amplitude of these oscillations appears 
consistent over the years, indicating a stable seasonal effect without 
significant year-over-year changes in peak or trough temperatures.
From a visual inspection, there are no apparent outliers or disruptions in the 
seasonal pattern, suggesting that the dataset is clean and well-maintained. 
The regularity of the pattern would likely make it suitable for forecasting 
using seasonal models, such as SARIMA or TBATS, which could exploit the 
periodicity inherent in the data.
```{r, echo=FALSE}
#The ACF and Pacf tell us that there is ovbious and strong seasonality and some auto-regressive component up to about 6 lags.
P1 <-Acf(temperature_ts, main = "ACF on original data")
```
The graph illustrates the Autocorrelation Function (ACF) applied to the original temperature data set. Autocorrelation is a mathematical representation of the 
degree of similarity between a given time series and a lagged version of itself 
over successive time intervals. The ACF is plotted against various lag values, 
which span from 0 to 24.
The y-axis represents the autocorrelation coefficient, ranging from -0.5 to 0.5, 
while the x-axis represents the lag in terms of the number of time units. Each 
vertical bar corresponds to the autocorrelation coefficient at a specific lag.
Notably, the graph shows a pattern of spikes at regular intervals, which 
suggests a seasonal pattern in the data. The presence of these spikes at 
consistent intervals can be indicative of the seasonality in the dataset, which correlates with the seasonal fluctuations seen in the time series plot of 
the original dataset.
The blue dashed lines represent the significance bounds. Any spike that extends 
beyond these bounds is considered statistically significant. In this ACF plot, 
several lags have autocorrelation values that cross the significance threshold,
confirming that the data exhibit a strong seasonal component at these lags.

```{r, echo=FALSE}}
P2 <- Pacf(temperature_ts, main = "PACF on original data")
```
In this PACF plot, most of the spikes are within the significance bounds, 
suggesting that most of the autocorrelations in the original data can be 
accounted for by the immediate preceding values.

```{r, echo=FALSE}
decomposed_Temperature <- decompose(temperature_ts, type = "additive")
plot(decomposed_Temperature)

#Testing if the trend is stationary
MKTest <- MannKendall(temperature_ts)
print(summary(MKTest))

print(adf.test(temperature_ts, alternative = "stationary"))
```
The Augmented Dickey-Fuller Test result indicates a Dickey-Fuller statistic of 
-11.153 with a lag order of 5, and a p-value of 0.01, suggesting that the null 
hypothesis of a unit root can be rejected and the time series is stationary.
```{r, echo=FALSE}
#confirming that timeseries transformation works as expected.
#str(temperature_ts)
```

For the forecasting purposes, the data was split into training and test data
following a proportion of 80/20.
```{r, echo=FALSE}
#Making Training and Test sets
split_point <- round(length(temperature_ts) * 0.8)
train_ts <- window(temperature_ts, end=c(2019, 8))
test_ts <- window(temperature_ts, start=c(2019, 9))
```

```{r, echo=FALSE}
#Confirms Training and Test sets were correctly made
#start(train_ts)
#end(train_ts)
#start(test_ts)
#end(test_ts)
```

*SARIMA Modeling*
```{r, echo=FALSE}
#SARIMA autofit
sarima_model <- auto.arima(train_ts)
print(summary(sarima_model))
```

```{r, echo=FALSE}
checkresiduals(sarima_model)
```

```{r, echo=FALSE}
h <- length(test_ts)
sarima_forecast <- forecast(sarima_model, h=h)
```

```{r, echo=FALSE}
autoplot(temperature_ts, series = "Original") +
  autolayer(sarima_forecast$mean, series = "SARIMA forecast") +
  ylab("Water Temperature") +
  ggtitle("SARIMA modeling")
```

```{r, echo=FALSE}
checkresiduals(sarima_forecast)
```

```{r, echo=FALSE}
forecast_accuracySarima <- accuracy(sarima_forecast)
print(forecast_accuracySarima)
```

*ARIMA+fourier Modeling*
```{r ARIMA, echo=TRUE, message=FALSE, warning=FALSE}
ARIMA_Four_fit <- auto.arima(train_ts, 
                             seasonal=TRUE, 
                             lambda=0,
                             xreg=fourier(train_ts, 
                                          K=c(6))
                             )

ARIMA_Four_for <- forecast(ARIMA_Four_fit,
                           xreg=fourier(train_ts,
                                        K=c(6),
                                        h=h),
                           h=h
                           ) 

autoplot(ARIMA_Four_for) + ylab("Temperature")

autoplot(temperature_ts) +
  autolayer(ARIMA_Four_fit$fitted, series="ARIMA_FOURIER Fitted",PI=FALSE) +
  autolayer(ARIMA_Four_for, series="ARIMA_FOURIER Forecast",PI=FALSE) +
  ylab("Temperature")

checkresiduals(ARIMA_Four_for)

forecast_accuracyARIMA_Four <- accuracy(ARIMA_Four_for)
print(forecast_accuracyARIMA_Four)
```

*STL + ETS Modeling*

```{r, echo=FALSE}
# STL + ETS
ets_model <- stlf(train_ts, h=h)
```

```{r, echo=FALSE}
ETS_for <- forecast(ets_model, h=h)

autoplot(train_ts) +
    autolayer(ETS_for, series = "ETS Forecast", PI = FALSE) +
  autolayer(fitted(ets_model), series = "Model STL + ETS", PI=FALSE) +
    autolayer(test_ts, series = "Test Data", PI=FALSE) +
  ylab("Water Temperature") +
  ggtitle("STL + ETS modeling")

```

```{r, echo=FALSE}
stlf_accuracy <- accuracy(ETS_for)
print(stlf_accuracy)
```

```{r, echo=FALSE}
checkresiduals(ets_model)
```

*TBATS Modeling*

```{r, echo=FALSE}
model_tbats <- tbats(train_ts)
print(summary(model_tbats))
```

```{r, echo=FALSE}
checkresiduals(model_tbats)
```


```{r, echo=FALSE}
TBATS_for <- forecast(model_tbats, h=h)

autoplot(train_ts) +
  autolayer(TBATS_for, series = "TBATS Forecast", PI = FALSE) +
  autolayer(test_ts, series = "Test Data", PI = FALSE) +
  autolayer(fitted(model_tbats), series = "Model", PI = FALSE) +
  ylab("Water Temperature") +
  ggtitle("TBATS Modeling")

```
```{r, echo=FALSE}
TBATS_accuracy <- accuracy(TBATS_for)
print(TBATS_accuracy)
```

*SSES*
```{r, echo=FALSE}
SSES_seas <- es(train_ts,model="ZZZ", h= h,holdout=FALSE)
plot(SSES_seas)
checkresiduals(SSES_seas)

#Plot model + observed data
autoplot(temperature_ts) +
  autolayer(SSES_seas$fitted, series="SSES Fit",PI=FALSE)+
  autolayer(SSES_seas$forecast, series="SSES Forecast",PI=FALSE)+
  ylab("Temperature") 

forecast_accuracy_SSES <- accuracy(SSES_seas)
```

*Neural Network*
```{r, echo=FALSE}
NN_fit <- nnetar(train_ts,
                 p=1,
                 P=1,
                 xreg=fourier(train_ts, K=c(6)))

NN_for <- forecast(NN_fit, h= h,xreg=fourier(train_ts, 
                                          K=c(6),h= h))

autoplot(NN_for) +
  ylab("Temperature C") 

autoplot(test_ts) +
  autolayer(NN_for, series="Neural Network",PI=FALSE)+
  ylab("Temperature") 

autoplot(temperature_ts) +
  autolayer(NN_for$fitted, series="NN fit",PI=FALSE) +
  autolayer(NN_for$mean, series="NN forecast",PI=FALSE)+
  ylab("Temperature") 

checkresiduals(NN_fit)

forecast_accuracy_NN <- accuracy(NN_for)
```
*Create Scores*
```{r, echo=FALSE}

#Model 1: Sarima
Sarima_scores <- accuracy(sarima_forecast$mean, test_ts) 

#Model 2: ARIMA + Fourier
ArimaFour_scores <- accuracy(ARIMA_Four_for$mean, test_ts)  

#Model 3: ETS+STL 
stlf_scores <- accuracy(ETS_for$mean, test_ts)

# Model 4:  TBATS 
TBATS_scores <- accuracy(TBATS_for$mean, test_ts)

# Model 5:  SSES 
SSES_scores <- accuracy(NN_for$mean, test_ts)

# Model 6:  Neural Network 
NN_scores <- accuracy(SSES_seas$forecast, test_ts)

```

*Create Score table*
```{r, echo=FALSE}
scores <- as.data.frame(
  rbind(Sarima_scores, ArimaFour_scores, stlf_scores, TBATS_scores, SSES_scores, NN_scores)
  )
row.names(scores) <- c("Sarima", "ARIMA+Fourier","ETS+STL","TBATS","SSES", "NN")

best_model_index <- which.min(scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(scores[best_model_index,]))                       
```

